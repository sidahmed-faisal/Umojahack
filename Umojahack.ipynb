{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTZATqKuzQud"
      },
      "source": [
        "<h2><center> Welcome to the Faulty Air-Quality Sensor Challenge</h2></center>\n",
        "<figure>\n",
        "<center><img src =\"https://drive.google.com/uc?export=view&id=1hSOAfRhJ_jo-MZAjq81VYJu5bZNL7EjD\" width = \"800\" height = '500'/>\n",
        "\n",
        "*About the problem*\n",
        "> AirQo’s air quality sensing network has more than 120 low-cost devices deployed across Uganda; in most cases, these devices are deployed in unmonitored or perilous environments. These low-cost electronic devices are susceptible to breakdown caused by communication malfunction, aging, wear and tear, manufacturing deficiencies, incorrect calibration, mishandling and other external environmental factors. Faults lead to data inaccuracies and data loss, which impacts decisions and policies that could significantly impact people’s lives. \n",
        "\n",
        "*Objective of this challenge*\n",
        "> In this challenge, your task is to develop a binary clasification model to identify faulty sensors, regardless of the device type. The model will be used by AirQo to automatically flag devices that have faulty readings. \n",
        "\n",
        "Device failure detection and monitoring is critical to AirQo’s work; faulty devices need to be identified, isolated and fixed or replaced with urgency\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNwjd3IWlgxc"
      },
      "source": [
        "## Table of contents:\n",
        "\n",
        "1. [Importing libraries](#Libraries)\n",
        "2. [Loading data](#Data)\n",
        "3. [Statistical summaries](#Statistics)\n",
        "4. [Missing values and duplicates](#Missing)\n",
        "5. [Outliers](#Outliers)\n",
        "6. [Feature engineering](#Engineering)\n",
        "7. [Date features EDA](#Dates)\n",
        "8. [Correlations - EDA](#Correlations)\n",
        "9. [Preprocess test dataset](#Preprocess)\n",
        "10. [Modelling](#Modelling)\n",
        "11. [Making predictions of the test set and creating a submission file](#Predictions)\n",
        "12. [TO DOs](#Tips)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Kdz7a25j4P"
      },
      "source": [
        "<a name=\"Libraries\"></a>\n",
        "## 1. Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJvGrR3AXhny"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "pd.options.display.float_format = '{:.5f}'.format\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-KRpqn5pbB"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "## 2. Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "V54N2h-CXzmI",
        "outputId": "58daf9fc-c6b0-41af-df11-15ea6c02fde1"
      },
      "outputs": [],
      "source": [
        "# Load files\n",
        "train = pd.read_csv('train.csv', parse_dates = ['Datetime'])\n",
        "test = pd.read_csv('test.csv', parse_dates = ['Datetime'])\n",
        "samplesubmission = pd.read_csv('SampleSubmission.csv')\n",
        "\n",
        "# Preview train dataset\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPMVj-JXXzh-"
      },
      "outputs": [],
      "source": [
        "# Preview test dataset\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V08kByzvZcmU"
      },
      "outputs": [],
      "source": [
        "# Preview sample submission file\n",
        "samplesubmission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdoDJBvdXzf4"
      },
      "outputs": [],
      "source": [
        "# Check size and shape of datasets\n",
        "train.shape, test.shape, samplesubmission.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNtwMcKjZk9x"
      },
      "source": [
        "<a name=\"Statistics\"></a>\n",
        "## 3. Statistical summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4NqRM3yXzeE"
      },
      "outputs": [],
      "source": [
        "# Train statistical summary\n",
        "train.describe(include = 'all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kESZ-qvnSEa"
      },
      "source": [
        "From the above statistical summary, we can deduce some of the following insights:\n",
        " - The train data provided ranges from *2021-10-15 16:00:34* to *2022-01-21 07:34:57*\n",
        " - There is a high correlation between Sensor1_PM2.5\tand Sensor2_PM2.5\n",
        " - Minimum recorded temperature is *16.70000* and a maximum *34.90000*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iTls3u3S6Yo"
      },
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(train[\"Offset_fault\"])\n",
        "plt.title('Target variable distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4gly1_uSAeY"
      },
      "source": [
        "The target variable is not well balanced.\n",
        "Some of the techniques to handle imbalanceness include:\n",
        "- Smote\n",
        "- Oversampling\n",
        "- Undersampling ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJNhVNKn5uEE"
      },
      "source": [
        "<a name=\"Missing\"></a>\n",
        "## 4. Missing values and duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ftu9Y3tbXzk"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "train.isnull().sum().any(), test.isnull().sum().any() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3eu41vhbXw9"
      },
      "outputs": [],
      "source": [
        "# Plot missing values in train set\n",
        "ax = train.isna().sum().sort_values().plot(kind = 'barh', figsize = (9, 10))\n",
        "plt.title('Percentage of Missing Values Per Column in Train Set', fontdict={'size':15})\n",
        "for p in ax.patches:\n",
        "    percentage ='{:,.0f}%'.format((p.get_width()/train.shape[0])*100)\n",
        "    width, height =p.get_width(),p.get_height()\n",
        "    x=p.get_x()+width+0.02\n",
        "    y=p.get_y()+height/2\n",
        "    ax.annotate(percentage,(x,y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06doaah27vk"
      },
      "source": [
        "Suggestions on how to handle missing values:\n",
        " - Fill in missing values with mode, mean, median..\n",
        " - Drop Missing datapoints with missing values\n",
        " - Fill in with a large number e.g -999999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5XFZniUMZ3O"
      },
      "outputs": [],
      "source": [
        "# Fill missing values with zeros\n",
        "train = train.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mALOf0CfbXs0"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "train.duplicated().any(), test.duplicated().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rty_X2Ov4fOy"
      },
      "source": [
        "No duplictes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VmqiddbckaX"
      },
      "source": [
        "<a name=\"Outliers\"></a>\n",
        "## 5. Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v4vaXv3bXqh"
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for each of the numerical columns\n",
        "sns.set_style('darkgrid')\n",
        "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (15, 10))\n",
        "fig.suptitle('Box plots showing outliers', y= 0.93, fontsize = 15)\n",
        "\n",
        "for ax, data, name in zip(axes.flatten(), train, ['Sensor1_PM2.5',\t'Sensor2_PM2.5',\t'Temperature',\t'Relative_Humidity']):\n",
        "  sns.boxplot(train[name], ax = ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERPdXhEx2pjD"
      },
      "source": [
        "Outliers are those data points which differs significantly from other observations present in given dataset.\n",
        "\n",
        "Suggestions on how to handle outliers:\n",
        " - Transforming the outliers by scaling - log transformation, box-cox transformation ...\n",
        " - Dropping outliers\n",
        " - Imputation by replacing outliers with mean, median ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndwt2QXVpyXF"
      },
      "source": [
        "<a name=\"Engineering\"></a>\n",
        "## 6. Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdt5924PbXoY"
      },
      "outputs": [],
      "source": [
        "# Extract day, month year and hour from the Datetime column\n",
        "# day\n",
        "train['Datetime_day'] = train.Datetime.dt.day\n",
        "\n",
        "# month\n",
        "train['Datetime_month'] = train.Datetime.dt.month\n",
        " \n",
        "# year\n",
        "train['Datetime_year'] = train.Datetime.dt.year\n",
        "\n",
        "# hour\n",
        "train['Datetime_hour'] = train.Datetime.dt.hour\n",
        "\n",
        "# Preview engineered date features\n",
        "train[['Datetime', 'Datetime_day', 'Datetime_month', 'Datetime_year', 'Datetime_hour']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY_lY3A2w5Qb"
      },
      "source": [
        "<a name=\"Dates\"></a>\n",
        "## 7. Date features EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BM9cJepx03e"
      },
      "outputs": [],
      "source": [
        "# Datetime month countplot\n",
        "plt.figure(figsize = (14, 7))\n",
        "sns.countplot(x = 'Datetime_month', data = train)\n",
        "plt.title('Datetime month count plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGIt4f-esd9J"
      },
      "source": [
        "- The only months available in the train set include *Jan, Oct, Nov and Dec*\n",
        "- March has the least number of observations in the dataset while December has the highest number of observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLifvK1kx8GX"
      },
      "outputs": [],
      "source": [
        "# Box plots for Sensor1_PM2.5\tand Sensor2_PM2.5 vs Offset faults\n",
        "sns.set_style('darkgrid')\n",
        "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (17, 7))\n",
        "fig.suptitle('Box plot for Sensor1_PM2.5\tand Sensor2_PM2.5 vs Offset faults', y= 0.95, fontsize = 15)\n",
        "\n",
        "for ax, data, name in zip(axes.flatten(), train, ['Sensor1_PM2.5', 'Sensor2_PM2.5']):\n",
        "  sns.boxplot(train.Offset_fault, train[name], ax= ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwTKG14O1zo"
      },
      "source": [
        " - Sensor 1 and sensor 2 are highly correlated with majority of the non faulty observations having the highest number of outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9O2HykxLwcJ"
      },
      "source": [
        "<a name=\"Correlations\"></a>\n",
        "## 8. Correlations - EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvMDQtKDEb0M"
      },
      "outputs": [],
      "source": [
        "# Type of correlations \n",
        "plt.figure(figsize = (20, 12))\n",
        "num_cols = ['Sensor1_PM2.5',\t'Sensor2_PM2.5',\t'Temperature',\t'Relative_Humidity']\n",
        "sns.pairplot(train[num_cols], kind=\"scatter\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdwD4SJEPRBB"
      },
      "source": [
        " - There is a positive correlation between Sensor1_PM2.5\tSensor2_PM2.5\n",
        " - There is a negative correlation between temperature and humidity\n",
        " - There seems to be no correlation between sensor PM2.5 and temperature/humidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUKo9oVKCQMm"
      },
      "outputs": [],
      "source": [
        "# Quantify correlations\n",
        "corr = train.corr()\n",
        "plt.figure(figsize = (13, 8))\n",
        "sns.heatmap(corr, cmap='RdYlGn', annot = True, center = 0)\n",
        "plt.title('Correlogram', fontsize = 15, color = 'darkgreen')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxifxALcP_p1"
      },
      "source": [
        "- There is a 0.96 positive correlation between sensor1 PM2.5 and sensor2 PM2.5\n",
        "- There is a -0.99 negative correlation between temperature and humidity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b2HJYzArroH"
      },
      "source": [
        "<a name=\"Preprocess\"></a>\n",
        "## 9.  Preprocess test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELn_cwzCXzWw"
      },
      "outputs": [],
      "source": [
        "# Extract day, month and year from the Datetime column\n",
        "test['Datetime_day'] = test.Datetime.dt.day\n",
        "test['Datetime_month'] = test.Datetime.dt.month\n",
        "test['Datetime_year'] = test.Datetime.dt.year\n",
        "test['Datetime_hour'] = test.Datetime.dt.hour\n",
        "\n",
        "# Fill in missing values with Zeroes\n",
        "test = test.fillna(0)\n",
        "\n",
        "# Preview engineered test set\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gIDFBaMs8fk"
      },
      "source": [
        "<a name=\"Modelling\"></a>\n",
        "## 10.  Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3cCJUM2s4Q5"
      },
      "outputs": [],
      "source": [
        "# Selecting the independent variables and the target variable\n",
        "train_feats = ['Sensor1_PM2.5',\t'Sensor2_PM2.5',\t'Temperature',\t'Relative_Humidity',\t'Datetime_day',\\\n",
        "               'Datetime_month',\t'Datetime_year', 'Datetime_hour']\n",
        "\n",
        "X = train[train_feats].fillna(0)\n",
        "y = train.Offset_fault\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "# Instantiating the model\n",
        "clf = RandomForestClassifier(max_depth = 3, random_state = 0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Measuring the accuracy of the model\n",
        "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
        "print('\\n')\n",
        "print(f'{classification_report(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zZYaANIjRgJ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=2)\n",
        "X_train_res , y_train_res = sm.fit_resample(X_train,y_train.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBOYZhh0JE9L"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
        "fig, ax = plt.subplots(figsize=(15,7))\n",
        "disp.plot(ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UuShmECwAbL"
      },
      "source": [
        "<a name=\"Predictions\"></a>\n",
        "## 11. Making predictions of the test set and creating a submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuFxPcaNHBQM"
      },
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "impo_df = pd.DataFrame({'feature': X.columns, 'importance': clf.feature_importances_}).set_index('feature').sort_values(by = 'importance', ascending = False)\n",
        "impo_df = impo_df[:12].sort_values(by = 'importance', ascending = True)\n",
        "impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\n",
        "plt.legend(loc = 'center right')\n",
        "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\n",
        "plt.xlabel('Features', fontsize = 12, color = 'indigo')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTxjNDKlJXRI"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier \n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "# create models \n",
        "lg_model = LogisticRegression()\n",
        "rf_model = RandomForestClassifier()\n",
        "kn_model = KNeighborsClassifier()\n",
        "et_model = ExtraTreesClassifier()\n",
        "xg_model = XGBClassifier()\n",
        "lgb_model = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n",
        "\n",
        "\n",
        "#fitting the models\n",
        "# lg_model.fit(X_train,y_train)\n",
        "# rf_model.fit(X_train,y_train)\n",
        "# kn_model.fit(X_train,y_train)\n",
        "# et_model.fit(X_train,y_train)\n",
        "# xg_model.fit(X_train,y_train)\n",
        "lgb_model.fit(X_train_res,y_train_res,verbose=20,eval_metric='logloss')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhNAOgOJK4Ks"
      },
      "outputs": [],
      "source": [
        "# Making predictions\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Measuring the accuracy of the model\n",
        "print(f'Accuracy score: {accuracy_score(y_test, y_pred)}')\n",
        "print('\\n')\n",
        "print(f'{classification_report(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJo1lD5cs4MM"
      },
      "outputs": [],
      "source": [
        "# Make prediction on the test set\n",
        "test_df = test[train_feats]\n",
        "# test_df = test_df.sample(n=127361 , random_state=1)\n",
        "predictions = lgb_model.predict(test_df)\n",
        "\n",
        "# # Create a submission file\n",
        "sub_file = samplesubmission.copy()\n",
        "sub_file.Offset_fault = predictions\n",
        "\n",
        "# Check the distribution of your predictions\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(sub_file.Offset_fault)\n",
        "plt.title('Predictions Data Distribution');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjjKQchqRVq"
      },
      "source": [
        " - Majority of the model predictions are 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvzrU6nms4Jw"
      },
      "outputs": [],
      "source": [
        "# Create file\n",
        "sub_file.to_csv('submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKXXj1Yh4-ZK"
      },
      "source": [
        "<a name=\"Tips\"></a>\n",
        "##12. TO DOs\n",
        "\n",
        "1. Generate more features, you can work on aggregating the columns( e.g mean temperature , mean humidity  etc)\n",
        "2. For the datetime you can generate more features, day of the week, week of the year ...\n",
        "3. Perform more EDA to get a better Understanding of the data, \n",
        "4. Try other classifier models\n",
        "5. Experiment with different startegies of handling the missing values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_BBwmW05HAR"
      },
      "source": [
        "## ALL THE BEST! "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
